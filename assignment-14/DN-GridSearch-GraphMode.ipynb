{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/faizanahemad/eva/blob/master/assignment-14/DN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "- XLA\n",
    "\n",
    "    - [Google Notebook example](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/tutorials/xla_compile.ipynb)\n",
    "\n",
    "    - [Blog](https://medium.com/@xianbao.qian/use-xla-with-keras-3ca5d0309c26)\n",
    "    - [Example Enabling XLA](https://github.com/tensorflow/models/blob/7212436440eaa11293ca84befcc5d8327109ea76/official/utils/misc/keras_utils.py#L158)\n",
    "    \n",
    "- Mixed Precision\n",
    "\n",
    "- Augmentation Libs\n",
    "    - [imgaug](https://github.com/aleju/imgaug)\n",
    "    - [albumentations](https://github.com/albu/albumentations)\n",
    "    - [Automold](https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library)\n",
    "    - [Tensorflow Examples](https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/)\n",
    "    - [PIL ImageOps Examples](https://hhsprings.bitbucket.io/docs/programming/examples/python/PIL/ImageOps.html)\n",
    "    \n",
    "- Tensorflow References\n",
    "    - [Using Numpy functions](https://www.tensorflow.org/api_docs/python/tf/numpy_function)\n",
    "    - [Using Python functions](https://www.tensorflow.org/api_docs/python/tf/py_function)\n",
    "    - [Tensorflow Data](https://www.tensorflow.org/datasets/catalog/overview)\n",
    "    - [Dataset to TFRecord](https://github.com/tensorflow/tensorflow/issues/16926)\n",
    "    - [TFRecord](https://www.tensorflow.org/tutorials/load_data/tf_records#tfrecords_format_details)\n",
    "    - [TFRecord Load](https://www.tensorflow.org/tutorials/load_data/images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LfLeMAyQU7z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "jobs = int(os.cpu_count()/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "# tf.keras.backend.set_epsilon(1e-4)\n",
    "# tf.keras.backend.set_floatx('float16')\n",
    "\n",
    "# tf.random.set_random_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOsYqWHvQBrZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.0\n",
      "0.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0824 13:35:37.386745 140608712148800 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/eva/assignment-14/lib.py:207: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "W0824 13:35:37.387819 140608712148800 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/eva/assignment-14/lib.py:209: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "print(np.__version__)\n",
    "print(skimage.__version__)\n",
    "\n",
    "\n",
    "import time, math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import gc\n",
    "print(tf.__version__)\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import copy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import os\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "from data_science_utils import misc\n",
    "from data_science_utils.vision.keras.regularizers import get_cutout_eraser\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "from matplotlib import cm\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,RandomFog,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, ChannelDropout, ChannelShuffle,RandomContrast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0824 13:35:37.505244 140608712148800 <ipython-input-4-bf20c9c65a9e>:1] This is a warning\n"
     ]
    }
   ],
   "source": [
    "logger.info('This is a warning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_append_to_results(filename, result_object=None, read=False):\n",
    "    from pathlib import Path\n",
    "    import ast\n",
    "    my_file = Path(filename)\n",
    "    if my_file.is_file():\n",
    "        results = misc.load_list_per_line(filename)\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    results = list(filter(lambda x:len(x)>2,results))\n",
    "    \n",
    "    if result_object is not None:\n",
    "        results.append(result_object)\n",
    "        misc.save_list_per_line(results, filename)\n",
    "        \n",
    "    if read:\n",
    "        def lit_eval(r):\n",
    "            try:\n",
    "                return ast.literal_eval(r)\n",
    "            except:\n",
    "                return None\n",
    "        return list(filter(lambda x: x is not None,map(lit_eval,results)))\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,BATCH_SIZE,EPOCHS,train_dataset,train_dataset_one_epoch,x_test, y_test,len_train,log_test_acc=False):\n",
    "    len_test = len(x_test)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE).prefetch(len_test).cache()\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
    "    logger.debug(\"Starting Training\")\n",
    "\n",
    "    model.fit(train_dataset, epochs=EPOCHS,steps_per_epoch=20)\n",
    "    \n",
    "    test_res = model.evaluate(test_dataset)\n",
    "    train_res = model.evaluate(train_dataset_one_epoch)\n",
    "\n",
    "    print(train_res,test_res)\n",
    "    t = time.time()\n",
    "              \n",
    "    time_spent = time.time() - t\n",
    "    logger.debug(msg(\"Train acc = \",train_accs[-1],\"Test acc =\",test_accs[-1],\"Time Taken = \",time_spent))\n",
    "    return train_accs[-1],test_accs[-1],time_spent\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self,lr_function,momentum_function):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.lr_function = lr_function\n",
    "        self.momentum_function = momentum_function\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        scheduled_lr = self.lr_function(batch)\n",
    "        scheduled_momentum = self.momentum_function(batch)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.momentum, scheduled_momentum)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_builder(model_config,BATCH_SIZE, len_train):\n",
    "    \n",
    "    batches_per_epoch = len_train//BATCH_SIZE + 1\n",
    "    model = FNet(**model_config[\"model\"])\n",
    "    \n",
    "    enable_olr = model_config[\"optimizer\"][\"enable_olr\"]\n",
    "    enable_momentum = enable_olr\n",
    "    max_lr = model_config[\"optimizer\"][\"max_lr\"]\n",
    "    max_momentum = model_config[\"optimizer\"][\"max_momentum\"]\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    if enable_olr:\n",
    "        mid_epoch = model_config[\"optimizer\"][\"mid_epoch\"]\n",
    "        start_lr = model_config[\"optimizer\"][\"start_lr\"]\n",
    "        \n",
    "        end_lr = model_config[\"optimizer\"][\"end_lr\"]\n",
    "        pre_end_lr = model_config[\"optimizer\"][\"pre_end_lr\"]\n",
    "        pre_end_epoch = model_config[\"optimizer\"][\"pre_end_epoch\"]\n",
    "        \n",
    "        enable_momentum = model_config[\"optimizer\"][\"enable_momentum\"]\n",
    "        if enable_momentum:\n",
    "            min_momentum = model_config[\"optimizer\"][\"min_momentum\"]\n",
    "            momentum_schedule =  lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [max_momentum, min_momentum, max_momentum, max_momentum])[0] \n",
    "            momentum_func = lambda batch: momentum_schedule(batch/batches_per_epoch)\n",
    "    \n",
    "        lr_schedule = lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [start_lr, max_lr, pre_end_lr, end_lr])[0] # LR = 0.75\n",
    "        lr_func = lambda batch: lr_schedule(batch/batches_per_epoch)\n",
    "    \n",
    "    lrs = LearningRateScheduler(lr_function=lr_func,momentum_function=momentum_func)\n",
    "\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=opt,loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'],callbacks=[lrs])\n",
    "    logger.debug(\"Model Built\")\n",
    "    return model,opt,global_step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenters(da_config,train_mean):\n",
    "    cutout_config = da_config[\"cutout_config\"]\n",
    "    cifar10_augs_config = da_config[\"cifar10_augs_config\"]\n",
    "    hue_config = da_config[\"hue_config\"]\n",
    "    cutout_mapper = get_numpy_wrapper(get_cutout_eraser(**cutout_config),\"cutout\")\n",
    "    cifar10_mapper = get_numpy_wrapper(CIFAR10Policy(**cifar10_augs_config, fillcolor=tuple(train_mean.astype(int)), log=False),\"AutoAug\")\n",
    "    hue_mapper = get_hue_aug(hue_config[\"max_delta\"])\n",
    "    caster = lambda x,y: (tf.cast(x,tf.float32),tf.cast(y,tf.int64))\n",
    "#     [hflip_mapper,cutout_mapper,cifar10_mapper,hue_mapper,caster]\n",
    "    full_wrapper = get_multimapper([])\n",
    "    logger.debug(\"Augmentation Functions Built\")\n",
    "    return full_wrapper\n",
    "\n",
    "\n",
    "def process_full_augmentation_all_epochs(full_wrapper, EPOCHS,BATCH_SIZE, normalize, x_train, y_train):\n",
    "    train_data = {}\n",
    "    len_train = len(x_train)\n",
    "    normalize = get_numpy_wrapper(normalize, Tout=tf.float32)\n",
    "    caster = lambda x,y: (tf.cast(x,tf.float32),tf.cast(y,tf.int64))\n",
    "    normalize = get_multimapper([])\n",
    "    t = time.time()\n",
    "    train_set_one_epoch = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(full_wrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE).prefetch(len_train)\n",
    "    train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(full_wrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(len_train).batch(len_train).prefetch(len_train)\n",
    "    return train_set,train_set_one_epoch,1\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(full_wrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(len_train).batch(len_train).prefetch(len_train)\n",
    "#         i = 0\n",
    "#         for x,y in train_set:\n",
    "#             xt = x.numpy()\n",
    "#             logger.debug(msg(\"X Shape = \",xt.shape[0], \"Train Length =\",len_train))\n",
    "#             assert x.numpy().shape[0]==len_train\n",
    "#             train_set = (xt,y.numpy())\n",
    "#             i = i+1\n",
    "#         assert i==1\n",
    "        train_data[epoch] = train_set\n",
    "        logger.debug(msg(\"Augmentation Epoch = \",epoch,\"Time Spent = %.1f\" % (time.time() - t)))\n",
    "        \n",
    "    \n",
    "    train_set = tf.data.Dataset.from_tensor_slices(train_data[0])\n",
    "    for epoch in range(1,EPOCHS):\n",
    "        key = (epoch)%(max(train_data.keys())+1)\n",
    "        ts = tf.data.Dataset.from_tensor_slices(train_data[key])\n",
    "        train_set = train_set.concatenate(ts)\n",
    "    train_set.batch(BATCH_SIZE).prefetch(len_train).shuffle(len_train).cache()\n",
    "    time_taken = time.time() - t\n",
    "    logger.info(msg(\"Augmentation Done for EPOCHS = %s, Time Taken = %.1f\"%(EPOCHS,time_taken)))\n",
    "    return train_set,train_set_one_epoch,time_taken\n",
    "    \n",
    "    \n",
    "\n",
    "def fetch_cifar_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "    len_train, len_test = len(x_train), len(x_test)\n",
    "    y_train = y_train.astype('int64').reshape(len_train)\n",
    "    y_test = y_test.astype('int64').reshape(len_test)\n",
    "    train_mean = np.mean(x_train, axis=(0,1,2))\n",
    "    train_std = np.std(x_train, axis=(0,1,2))\n",
    "\n",
    "    def normalize(x):\n",
    "        return ((x.astype('float32') - train_mean) / train_std).astype('float32')\n",
    "\n",
    "    \n",
    "    pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
    "    x_train = pad4(x_train)\n",
    "    x_test = normalize(x_test)\n",
    "    logger.info(\"Data Fetching Done\")\n",
    "    return x_train, y_train, x_test, y_test, classes,len_train,len_test, normalize, train_mean\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def runner(configs,cv=3):\n",
    "    # fetch Data\n",
    "    x_train, y_train, x_test, y_test, classes,len_train,len_test, normalize,train_mean = fetch_cifar_data()\n",
    "    prev_da_config = None\n",
    "    train_data = None\n",
    "    results = []\n",
    "    logger.info(\"Started GridSearch, Total Configs to Test = %s\"%len(configs))\n",
    "    reset_keras()\n",
    "    t = time.time()\n",
    "    prev_epochs = 0\n",
    "    for i,config in enumerate(configs):\n",
    "        \n",
    "        _ = gc.collect()\n",
    "        model_config = config[\"model_config\"]\n",
    "        da_config = config[\"augmentation_config\"]\n",
    "        BATCH_SIZE = config[\"training_config\"][\"BATCH_SIZE\"]\n",
    "        EPOCHS = config[\"training_config\"][\"EPOCHS\"]\n",
    "        save_file = config[\"training_config\"][\"save_file\"]\n",
    "        \n",
    "        \n",
    "        same_as_previous_config = prev_da_config == da_config and EPOCHS==prev_epochs\n",
    "        prev_da_config = da_config\n",
    "        prev_epochs = EPOCHS\n",
    "        \n",
    "        if not same_as_previous_config:\n",
    "            augmenter = build_augmenters(da_config,train_mean)\n",
    "            train_set,train_set_one_epoch,time_taken = process_full_augmentation_all_epochs(augmenter, EPOCHS,BATCH_SIZE, normalize, x_train, y_train)\n",
    "            \n",
    "        train_acc = test_acc = time_spent = 0.0\n",
    "        for j in range(cv):\n",
    "            model,opt,global_step = model_builder(model_config,BATCH_SIZE, len_train)\n",
    "            t1_acc,t2_acc,tsp = run_model(model,BATCH_SIZE,EPOCHS,train_set,train_set_one_epoch,x_test, y_test,len_train,log_test_acc=False)\n",
    "            train_acc += t1_acc\n",
    "            test_acc += t2_acc\n",
    "            time_spent += tsp\n",
    "            reset_keras()\n",
    "        train_acc /= cv\n",
    "        test_acc /= cv\n",
    "        time_spent /= cv\n",
    "        \n",
    "        logger.info(msg(\"Epoch =\",i,\"Train acc = \",train_acc,\"Test acc =\",test_acc,\"Time Taken = \",time_spent))\n",
    "            \n",
    "        \n",
    "        result = dict(config=config,results=dict(train=train_acc,test=test_acc,training_time=time_spent,augmentation_time=time_taken))\n",
    "        read_and_append_to_results(save_file, result)\n",
    "        results.append(result)\n",
    "        del model\n",
    "        model = None\n",
    "    logger.info(\"Grid Search Complete, Total Results Count = %s, Time Taken = %.1f\",len(results),(time.time()-t))\n",
    "    assert len(results)==len(configs)\n",
    "    return results\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_and_append_to_results(\"results.txt\", read=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0824 13:36:39.452653 140608712148800 <ipython-input-15-a595619e1ae4>:67] Data Fetching Done\n",
      "I0824 13:36:39.453457 140608712148800 <ipython-input-15-a595619e1ae4>:83] Started GridSearch, Total Configs to Test = 1\n",
      "W0824 13:36:45.266106 140608712148800 training_utils.py:1300] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n\n    TypeError: tf__call() missing 1 required positional argument: 'y'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-adf4fe251798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_config_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a595619e1ae4>\u001b[0m in \u001b[0;36mrunner\u001b[0;34m(configs, cv)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mt1_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtsp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_set_one_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_test_acc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt1_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mtest_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt2_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-6aece9f1f295>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, BATCH_SIZE, EPOCHS, train_dataset, train_dataset_one_epoch, x_test, y_test, len_train, log_test_acc)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting Training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         shuffle=shuffle)\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2556\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2558\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2559\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2560\u001b[0m       \u001b[0my_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2774\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_training_arg\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2775\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2776\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2777\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_layer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n\n    TypeError: tf__call() missing 1 required positional argument: 'y'\n"
     ]
    }
   ],
   "source": [
    "augmentation_config = dict(cutout_config=dict(s_l=0.04, s_h=0.06,max_erasures_per_image=1),\n",
    "                           hue_config=dict(max_delta=0.05),\n",
    "                           cifar10_augs_config=dict(proba=0.5, enabled_policies=[(\"rotate\",5, 15,),(\"shearX\",0.05, 0.15,),(\"shearY\",0.05, 0.15,)]))\n",
    "\n",
    "model_config = dict(model=dict(start_kernels=64,sparse_bn=True,thin_block=False,\n",
    "                               enable_skip=True,enable_pool_before_skip=True),\n",
    "                    optimizer=dict(enable_olr=True,max_lr=0.6,\n",
    "                                   start_lr=0.01,pre_end_lr=0.04,end_lr=0.02,\n",
    "                                   mid_epoch=5,pre_end_epoch=13,\n",
    "                                   max_momentum=0.9,min_momentum=0.8, enable_momentum=True,))\n",
    "\n",
    "training_config = dict(BATCH_SIZE=512,EPOCHS=15,save_file=\"results.txt\")\n",
    "\n",
    "test_config_0 = dict(augmentation_config=augmentation_config,model_config=model_config,training_config=training_config)\n",
    "\n",
    "test_config_1 = copy.deepcopy(test_config_0)\n",
    "test_config_1[\"model_config\"][\"optimizer\"][\"mid_epoch\"] = 1\n",
    "test_config_1[\"model_config\"][\"optimizer\"][\"pre_end_epoch\"] = 2\n",
    "test_config_1[\"training_config\"][\"EPOCHS\"] = 3\n",
    "\n",
    "\n",
    "# mid_epoch\n",
    "\n",
    "# test_config_1 = copy.deepcopy(test_config_0)\n",
    "# test_config_0[\"augmentation_config\"][\"cutout_config\"][\"proba\"] = 0.9\n",
    "# test_config_0[\"augmentation_config\"][\"cutout_config\"][\"s_h\"] = 0.05\n",
    "\n",
    "# \n",
    "# test_config_2 = copy.deepcopy(test_config_0)\n",
    "# test_config_2[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.9\n",
    "# test_config_2[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.7\n",
    "\n",
    "# test_config_3 = copy.deepcopy(test_config_0)\n",
    "# test_config_3[\"model_config\"][\"optimizer\"][\"pre_end_epoch\"] = 14\n",
    "# test_config_3[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.9\n",
    "# test_config_3[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = runner([test_config_1],cv=1)\n",
    "\n",
    "\n",
    "# Next Test Confirm\n",
    "# Low LR - Low Momentum\n",
    "# High LR - High Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "DN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
