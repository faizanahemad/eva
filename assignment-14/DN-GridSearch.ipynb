{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/faizanahemad/eva/blob/master/assignment-14/DN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "- XLA\n",
    "\n",
    "    - [Google Notebook example](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/tutorials/xla_compile.ipynb)\n",
    "\n",
    "    - [Blog](https://medium.com/@xianbao.qian/use-xla-with-keras-3ca5d0309c26)\n",
    "    - [Example Enabling XLA](https://github.com/tensorflow/models/blob/7212436440eaa11293ca84befcc5d8327109ea76/official/utils/misc/keras_utils.py#L158)\n",
    "    \n",
    "- Mixed Precision\n",
    "\n",
    "- Augmentation Libs\n",
    "    - [imgaug](https://github.com/aleju/imgaug)\n",
    "    - [albumentations](https://github.com/albu/albumentations)\n",
    "    - [Automold](https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library)\n",
    "    - [Tensorflow Examples](https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/)\n",
    "    - [PIL ImageOps Examples](https://hhsprings.bitbucket.io/docs/programming/examples/python/PIL/ImageOps.html)\n",
    "    \n",
    "- Tensorflow References\n",
    "    - [Using Numpy functions](https://www.tensorflow.org/api_docs/python/tf/numpy_function)\n",
    "    - [Using Python functions](https://www.tensorflow.org/api_docs/python/tf/py_function)\n",
    "    - [Tensorflow Data](https://www.tensorflow.org/datasets/catalog/overview)\n",
    "    - [Dataset to TFRecord](https://github.com/tensorflow/tensorflow/issues/16926)\n",
    "    - [TFRecord](https://www.tensorflow.org/tutorials/load_data/tf_records#tfrecords_format_details)\n",
    "    - [TFRecord Load](https://www.tensorflow.org/tutorials/load_data/images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LfLeMAyQU7z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512 #@param {type:\"integer\"}\n",
    "MOMENTUM = 0.9 #@param {type:\"number\"}\n",
    "LEARNING_RATE = 0.5 #@param {type:\"number\"}\n",
    "WEIGHT_DECAY = 1e-3 #@param {type:\"number\"}\n",
    "EPOCHS = 13 #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "jobs = int(os.cpu_count()/2)\n",
    "\n",
    "float_rep = \"float16\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.keras.backend.set_epsilon(1e-4)\n",
    "# tf.keras.backend.set_floatx('float16')\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "# tf.random.set_random_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOsYqWHvQBrZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.0\n",
      "0.15.0\n",
      "1.14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/eva/assignment-14/lib.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "print(np.__version__)\n",
    "print(skimage.__version__)\n",
    "\n",
    "\n",
    "import time, math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import gc\n",
    "print(tf.__version__)\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import copy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import os\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "from data_science_utils import misc\n",
    "from data_science_utils.vision.keras.regularizers import get_cutout_eraser\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "from matplotlib import cm\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,RandomFog,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, ChannelDropout, ChannelShuffle,RandomContrast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0822 10:36:48.816156 140458372286272 <ipython-input-7-bf20c9c65a9e>:1] This is a warning\n"
     ]
    }
   ],
   "source": [
    "logger.info('This is a warning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_append_to_results(filename, result_object=None, read=False):\n",
    "    from pathlib import Path\n",
    "    import ast\n",
    "    my_file = Path(filename)\n",
    "    if my_file.is_file():\n",
    "        results = misc.load_list_per_line(filename)\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    results = list(filter(lambda x:len(x)>2,results))\n",
    "    \n",
    "    if result_object is not None:\n",
    "        results.append(result_object)\n",
    "        misc.save_list_per_line(results, filename)\n",
    "        \n",
    "    if read:\n",
    "        def lit_eval(r):\n",
    "            try:\n",
    "                return ast.literal_eval(r)\n",
    "            except:\n",
    "                return None\n",
    "        return list(filter(lambda x: x is not None,map(lit_eval,results)))\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,opt,global_step,BATCH_SIZE,EPOCHS,train_data,x_test, y_test,len_train,log_test_acc=False):\n",
    "    len_test = len(x_test)\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE).prefetch(len_test)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    logger.debug(\"Starting Training\")\n",
    "    t = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        tf.keras.backend.set_learning_phase(1)    \n",
    "        train_loss = test_loss = train_acc = test_acc = 0.0\n",
    "        train_set = train_data[epoch]\n",
    "        for (x, y) in train_set:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, correct = model(x, y)\n",
    "\n",
    "            var = model.trainable_variables\n",
    "            grads = tape.gradient(loss, var)\n",
    "            for g, v in zip(grads, var):\n",
    "                g += v * (WEIGHT_DECAY/(epoch+1)) * BATCH_SIZE\n",
    "            opt.apply_gradients(zip(grads, var), global_step=global_step)\n",
    "\n",
    "            train_loss += loss.numpy()\n",
    "            train_acc += correct.numpy()\n",
    "            train_accs.append(train_acc / len_train)\n",
    "        if log_test_acc or epoch==EPOCHS-1:    \n",
    "            tf.keras.backend.set_learning_phase(0)\n",
    "            for (x, y) in test_set:\n",
    "                loss, correct = model(x, y)\n",
    "                test_loss += loss.numpy()\n",
    "                test_acc += correct.numpy()\n",
    "            test_accs.append(test_acc / len_test)\n",
    "            logger.debug(msg(\"epoch = %2s\"%epoch,'||train=> loss: %.3f' %(train_loss / len_train), 'acc: %.3f' % (train_acc / len_train), '||val=> loss: %.3f' % (test_loss / len_test), 'val acc: %.3f' %(test_acc / len_test), '%.1fs'%(time.time() - t)))\n",
    "        logger.debug(msg(\"trained for epoch = \",epoch,\"train acc = \",train_accs[-1]))\n",
    "            \n",
    "    time_spent = time.time() - t\n",
    "    logger.debug(msg(\"Train acc = \",train_accs[-1],\"Test acc =\",test_accs[-1],\"Time Taken = \",time_spent))\n",
    "    return train_accs[-1],test_accs[-1],time_spent\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenters(da_config,train_mean):\n",
    "    cutout_config = da_config[\"cutout_config\"]\n",
    "    cifar10_augs_config = da_config[\"cifar10_augs_config\"]\n",
    "    hue_config = da_config[\"hue_config\"]\n",
    "    cutout_mapper = get_numpy_wrapper(get_cutout_eraser(**cutout_config),\"cutout\")\n",
    "    cifar10_mapper = get_numpy_wrapper(CIFAR10Policy(**cifar10_augs_config, fillcolor=tuple(train_mean.astype(int)), log=False),\"AutoAug\")\n",
    "    hue_mapper = get_hue_aug(hue_config[\"max_delta\"])\n",
    "    full_wrapper = get_multimapper([hflip_mapper,cutout_mapper,cifar10_mapper,hue_mapper])\n",
    "    logger.debug(\"Augmentation Functions Built\")\n",
    "    return full_wrapper\n",
    "\n",
    "\n",
    "def process_full_augmentation_all_epochs(full_wrapper, EPOCHS, normalize, x_train, y_train):\n",
    "    train_data = {}\n",
    "    len_train = len(x_train)\n",
    "    normalize = get_numpy_wrapper(normalize, Tout=tf.float32)\n",
    "    t = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(full_wrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(len_train).batch(len_train).prefetch(len_train)\n",
    "        i = 0\n",
    "        for x,y in train_set:\n",
    "            xt = x.numpy()\n",
    "            logger.debug(msg(\"X Shape = \",xt.shape[0], \"Train Length =\",len_train))\n",
    "            assert x.numpy().shape[0]==len_train\n",
    "            train_set = (xt,y.numpy())\n",
    "            i = i+1\n",
    "        assert i==1\n",
    "        train_data[epoch] = train_set\n",
    "        logger.debug(msg(\"Augmentation Epoch = \",epoch,\"Time Spent = %.1f\" % (time.time() - t)))\n",
    "        \n",
    "    train_sets = {}\n",
    "    for epoch in range(EPOCHS):\n",
    "        key = (epoch)%(max(train_data.keys())+1)\n",
    "        train_set = train_data[key]\n",
    "        train_set =tf.data.Dataset.from_tensor_slices(train_set).batch(BATCH_SIZE).prefetch(len_train)\n",
    "        train_sets[epoch] = train_set\n",
    "    time_taken = time.time() - t\n",
    "    logger.info(msg(\"Augmentation Done, Time Taken = %.1f\"%(time_taken)))\n",
    "    return train_sets,time_taken\n",
    "    \n",
    "    \n",
    "\n",
    "def fetch_cifar_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "    len_train, len_test = len(x_train), len(x_test)\n",
    "    y_train = y_train.astype('int64').reshape(len_train)\n",
    "    y_test = y_test.astype('int64').reshape(len_test)\n",
    "    train_mean = np.mean(x_train, axis=(0,1,2))\n",
    "    train_std = np.std(x_train, axis=(0,1,2))\n",
    "\n",
    "    def normalize(x):\n",
    "        return ((x.astype('float32') - train_mean) / train_std).astype('float32')\n",
    "\n",
    "    \n",
    "    pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
    "    x_train = pad4(x_train)\n",
    "    x_test = normalize(x_test)\n",
    "    logger.info(\"Data Fetching Done\")\n",
    "    return x_train, y_train, x_test, y_test, classes,len_train,len_test, normalize, train_mean\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "def model_builder(model_config,BATCH_SIZE, len_train):\n",
    "    \n",
    "    batches_per_epoch = len_train//BATCH_SIZE + 1\n",
    "    model = FNet(**model_config[\"model\"])\n",
    "    \n",
    "    enable_olr = model_config[\"optimizer\"][\"enable_olr\"]\n",
    "    enable_momentum = enable_olr\n",
    "    max_lr = model_config[\"optimizer\"][\"max_lr\"]\n",
    "    max_momentum = model_config[\"optimizer\"][\"max_momentum\"]\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    if enable_olr:\n",
    "        mid_epoch = model_config[\"optimizer\"][\"mid_epoch\"]\n",
    "        start_lr = model_config[\"optimizer\"][\"start_lr\"]\n",
    "        \n",
    "        end_lr = model_config[\"optimizer\"][\"end_lr\"]\n",
    "        pre_end_lr = model_config[\"optimizer\"][\"pre_end_lr\"]\n",
    "        pre_end_epoch = model_config[\"optimizer\"][\"pre_end_epoch\"]\n",
    "        \n",
    "        enable_momentum = model_config[\"optimizer\"][\"enable_momentum\"]\n",
    "        if enable_momentum:\n",
    "            min_momentum = model_config[\"optimizer\"][\"min_momentum\"]\n",
    "            momentum_schedule =  lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [max_momentum, min_momentum, max_momentum, max_momentum])[0] \n",
    "            momentum_func = lambda: momentum_schedule(global_step/batches_per_epoch)\n",
    "    \n",
    "        lr_schedule = lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [start_lr, max_lr, pre_end_lr, end_lr])[0] # LR = 0.75\n",
    "        lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE\n",
    "    \n",
    "    \n",
    "    \n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=lr_func if enable_olr else max_lr, momentum=momentum_func if enable_momentum else max_momentum, use_nesterov=True)\n",
    "    logger.debug(\"Model Built\")\n",
    "    return model,opt,global_step\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def runner(configs,cv=3):\n",
    "    # fetch Data\n",
    "    x_train, y_train, x_test, y_test, classes,len_train,len_test, normalize,train_mean = fetch_cifar_data()\n",
    "    prev_da_config = None\n",
    "    train_data = None\n",
    "    results = []\n",
    "    logger.info(\"Started GridSearch, Total Configs to Test = %s\"%len(configs))\n",
    "    reset_keras()\n",
    "    t = time.time()\n",
    "    for i,config in enumerate(configs):\n",
    "        \n",
    "        _ = gc.collect()\n",
    "        model_config = config[\"model_config\"]\n",
    "        da_config = config[\"augmentation_config\"]\n",
    "        BATCH_SIZE = config[\"training_config\"][\"BATCH_SIZE\"]\n",
    "        EPOCHS = config[\"training_config\"][\"EPOCHS\"]\n",
    "        save_file = config[\"training_config\"][\"save_file\"]\n",
    "        \n",
    "        same_as_previous_config = prev_da_config == da_config\n",
    "        prev_da_config = da_config\n",
    "        \n",
    "        \n",
    "        if not same_as_previous_config:\n",
    "            augmenter = build_augmenters(da_config,train_mean)\n",
    "            train_data,time_taken = process_full_augmentation_all_epochs(augmenter, EPOCHS, normalize, x_train, y_train)\n",
    "            \n",
    "        train_acc = test_acc = time_spent = 0.0\n",
    "        for i in range(cv):\n",
    "            model,opt,global_step = model_builder(model_config,BATCH_SIZE, len_train)\n",
    "            t1_acc,t2_acc,tsp = run_model(model,opt,global_step,BATCH_SIZE,EPOCHS,train_data,x_test, y_test,len_train,log_test_acc=False)\n",
    "            train_acc += t1_acc\n",
    "            test_acc += t2_acc\n",
    "            time_spent += tsp\n",
    "            reset_keras()\n",
    "        train_acc /= cv\n",
    "        test_acc /= cv\n",
    "        time_spent /= cv\n",
    "        \n",
    "        logger.debug(msg(\"Epoch =\",i,\"Train acc = \",train_acc,\"Test acc =\",test_acc,\"Time Taken = \",time_spent))\n",
    "            \n",
    "        \n",
    "        result = dict(config=config,results=dict(train=train_acc,test=test_acc,training_time=time_spent,augmentation_time=time_taken))\n",
    "        read_and_append_to_results(save_file, result)\n",
    "        results.append(result)\n",
    "        del model\n",
    "        model = None\n",
    "    logger.info(\"Grid Search Complete, Total Results Count = %s, Time Taken = %.1f\",len(results),(time.time()-t))\n",
    "    assert len(results)==len(configs)\n",
    "    return results\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_and_append_to_results(\"results.txt\", read=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "augmentation_config = dict(cutout_config=dict(s_l=0.04, s_h=0.06,max_erasures_per_image=1),\n",
    "                           hue_config=dict(max_delta=0.05),\n",
    "                           cifar10_augs_config=dict(proba=0.5, enabled_policies=[(\"rotate\",5, 15,),(\"shearX\",0.05, 0.15,),(\"shearY\",0.05, 0.15,)]))\n",
    "\n",
    "model_config = dict(model=dict(start_kernels=64,sparse_bn=True,thin_block=False,\n",
    "                               enable_skip=True,enable_pool_before_skip=True),\n",
    "                    optimizer=dict(enable_olr=True,max_lr=0.6,\n",
    "                                   start_lr=0.01,pre_end_lr=0.04,end_lr=0.02,\n",
    "                                   mid_epoch=5,pre_end_epoch=13,\n",
    "                                   max_momentum=0.9,min_momentum=0.8, enable_momentum=True,))\n",
    "\n",
    "training_config = dict(BATCH_SIZE=512,EPOCHS=15,save_file=\"results.txt\")\n",
    "\n",
    "test_config_0 = dict(augmentation_config=augmentation_config,model_config=model_config,training_config=training_config)\n",
    "\n",
    "test_config_1 = copy.deepcopy(test_config_0)\n",
    "test_config_1[\"model_config\"][\"optimizer\"][\"start_lr\"] = 0.01\n",
    "test_config_1[\"model_config\"][\"optimizer\"][\"pre_end_lr\"] = 0.04\n",
    "test_config_1[\"model_config\"][\"optimizer\"][\"end_lr\"] = 0.01\n",
    "\n",
    "\n",
    "test_config_2 = copy.deepcopy(test_config_0)\n",
    "test_config_2[\"model_config\"][\"optimizer\"][\"pre_end_epoch\"] = 12\n",
    "\n",
    "\n",
    "test_config_3 = copy.deepcopy(test_config_2)\n",
    "test_config_3[\"model_config\"][\"optimizer\"][\"start_lr\"] = 0.01\n",
    "test_config_3[\"model_config\"][\"optimizer\"][\"pre_end_lr\"] = 0.04\n",
    "test_config_3[\"model_config\"][\"optimizer\"][\"end_lr\"] = 0.01\n",
    "\n",
    "# \n",
    "test_config_4 = copy.deepcopy(test_config_0)\n",
    "test_config_4[\"model_config\"][\"optimizer\"][\"max_lr\"] = 0.4\n",
    "\n",
    "\n",
    "test_config_5 = copy.deepcopy(test_config_4)\n",
    "test_config_5[\"model_config\"][\"optimizer\"][\"start_lr\"] = 0.01\n",
    "test_config_5[\"model_config\"][\"optimizer\"][\"pre_end_lr\"] = 0.04\n",
    "test_config_5[\"model_config\"][\"optimizer\"][\"end_lr\"] = 0.01\n",
    "\n",
    "\n",
    "test_config_6 = copy.deepcopy(test_config_4)\n",
    "test_config_6[\"model_config\"][\"optimizer\"][\"pre_end_epoch\"] = 12\n",
    "\n",
    "\n",
    "test_config_7 = copy.deepcopy(test_config_6)\n",
    "test_config_7[\"model_config\"][\"optimizer\"][\"start_lr\"] = 0.01\n",
    "test_config_7[\"model_config\"][\"optimizer\"][\"pre_end_lr\"] = 0.04\n",
    "test_config_7[\"model_config\"][\"optimizer\"][\"end_lr\"] = 0.01\n",
    "\n",
    "# \n",
    "test_config_8 = copy.deepcopy(test_config_0)\n",
    "test_config_8[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.9\n",
    "test_config_8[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.7\n",
    "\n",
    "\n",
    "test_config_9 = copy.deepcopy(test_config_0)\n",
    "test_config_9[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.95\n",
    "test_config_9[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.75\n",
    "\n",
    "\n",
    "test_config_10 = copy.deepcopy(test_config_0)\n",
    "test_config_10[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.95\n",
    "test_config_10[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.85\n",
    "\n",
    "test_config_11 = copy.deepcopy(test_config_0)\n",
    "test_config_11[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.85\n",
    "test_config_11[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.75\n",
    "\n",
    "\n",
    "\n",
    "results = runner([test_config_0,test_config_1,test_config_2,test_config_3,\n",
    "                  test_config_4,test_config_5,test_config_6,test_config_7,\n",
    "                  test_config_8,test_config_9,test_config_10,test_config_11])\n",
    "\n",
    "\n",
    "# Next Test Confirm\n",
    "# Low LR - Low Momentum\n",
    "# High LR - High Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'train': 0.93056,\n",
       "   'test': 0.9161,\n",
       "   'training_time': 175.1889660358429,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.5,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92822,\n",
       "   'test': 0.9159,\n",
       "   'training_time': 174.78784084320068,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.5,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.9337,\n",
       "   'test': 0.9184,\n",
       "   'training_time': 174.5488088130951,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.5,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92862,\n",
       "   'test': 0.9154,\n",
       "   'training_time': 174.36740350723267,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.5,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92812,\n",
       "   'test': 0.9183,\n",
       "   'training_time': 175.19811820983887,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.4,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92362,\n",
       "   'test': 0.9133,\n",
       "   'training_time': 175.533855676651,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.4,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92862,\n",
       "   'test': 0.919,\n",
       "   'training_time': 175.16856694221497,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.4,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92436,\n",
       "   'test': 0.914,\n",
       "   'training_time': 175.74376702308655,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.4,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.93012,\n",
       "   'test': 0.9156,\n",
       "   'training_time': 175.04379510879517,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.6,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92802,\n",
       "   'test': 0.915,\n",
       "   'training_time': 174.8382875919342,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.6,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.93286,\n",
       "   'test': 0.9192,\n",
       "   'training_time': 174.91782236099243,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.6,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.93058,\n",
       "   'test': 0.9137,\n",
       "   'training_time': 175.23879170417786,\n",
       "   'augmentation_time': 533.4625051021576},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.6,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(result[\"results\"],result[\"config\"][\"model_config\"][\"optimizer\"]) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'train': 0.92972,\n",
       "   'test': 0.9152,\n",
       "   'training_time': 174.78745794296265,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.5,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92642,\n",
       "   'test': 0.9163,\n",
       "   'training_time': 175.32796812057495,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.5,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.93186,\n",
       "   'test': 0.9155,\n",
       "   'training_time': 175.3102626800537,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.5,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.9272,\n",
       "   'test': 0.9163,\n",
       "   'training_time': 175.4654312133789,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.5,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92964,\n",
       "   'test': 0.9185,\n",
       "   'training_time': 175.73435735702515,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.4,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92174,\n",
       "   'test': 0.9137,\n",
       "   'training_time': 175.4795949459076,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.4,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92784,\n",
       "   'test': 0.9185,\n",
       "   'training_time': 175.26103711128235,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.4,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.92338,\n",
       "   'test': 0.9122,\n",
       "   'training_time': 174.11331701278687,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.4,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.9312,\n",
       "   'test': 0.918,\n",
       "   'training_time': 174.75291681289673,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.6,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.9302,\n",
       "   'test': 0.9199,\n",
       "   'training_time': 175.1153838634491,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.6,\n",
       "   'start_lr': 0.005,\n",
       "   'pre_end_lr': 0.02,\n",
       "   'end_lr': 0.01,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.93282,\n",
       "   'test': 0.917,\n",
       "   'training_time': 175.2307116985321,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.6,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.9,\n",
       "   'min_momentum': 0.8,\n",
       "   'enable_momentum': True}),\n",
       " ({'train': 0.93082,\n",
       "   'test': 0.9181,\n",
       "   'training_time': 174.72894597053528,\n",
       "   'augmentation_time': 536.7730305194855},\n",
       "  {'enable_olr': True,\n",
       "   'max_lr': 0.6,\n",
       "   'start_lr': 0.01,\n",
       "   'pre_end_lr': 0.04,\n",
       "   'end_lr': 0.02,\n",
       "   'mid_epoch': 5,\n",
       "   'pre_end_epoch': 13,\n",
       "   'max_momentum': 0.85,\n",
       "   'min_momentum': 0.75,\n",
       "   'enable_momentum': True})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(result[\"results\"],result[\"config\"][\"model_config\"][\"optimizer\"]) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'config': {'augmentation_config': {'cutout_config': {'s_l': 0.04,\n",
       "     's_h': 0.06,\n",
       "     'max_erasures_per_image': 1},\n",
       "    'hue_config': {'max_delta': 0.1},\n",
       "    'cifar10_augs_config': {'proba': 0.5,\n",
       "     'enabled_policies': [('rotate', 5, 15),\n",
       "      ('shearX', 0.1, 0.2),\n",
       "      ('shearY', 0.1, 0.2)]}},\n",
       "   'model_config': {'model': {'start_kernels': 64,\n",
       "     'sparse_bn': True,\n",
       "     'thin_block': True},\n",
       "    'optimizer': {'enable_olr': True,\n",
       "     'max_lr': 0.5,\n",
       "     'start_lr': 0.001,\n",
       "     'pre_end_lr': 0.01,\n",
       "     'end_lr': 0.005,\n",
       "     'mid_epoch': 5,\n",
       "     'pre_end_epoch': 13,\n",
       "     'max_momentum': 0.9,\n",
       "     'enable_momentum': False}},\n",
       "   'training_config': {'BATCH_SIZE': 512,\n",
       "    'EPOCHS': 15,\n",
       "    'save_file': 'results.txt'}},\n",
       "  'results': {'train': 0.87154,\n",
       "   'test': 0.8858,\n",
       "   'training_time': 150.91348099708557,\n",
       "   'augmentation_time': 432.3216440677643}}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "DN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
