{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/faizanahemad/eva/blob/master/assignment-14/DN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "- XLA\n",
    "\n",
    "    - [Google Notebook example](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/tutorials/xla_compile.ipynb)\n",
    "\n",
    "    - [Blog](https://medium.com/@xianbao.qian/use-xla-with-keras-3ca5d0309c26)\n",
    "    - [Example Enabling XLA](https://github.com/tensorflow/models/blob/7212436440eaa11293ca84befcc5d8327109ea76/official/utils/misc/keras_utils.py#L158)\n",
    "    \n",
    "- Mixed Precision\n",
    "\n",
    "- Augmentation Libs\n",
    "    - [imgaug](https://github.com/aleju/imgaug)\n",
    "    - [albumentations](https://github.com/albu/albumentations)\n",
    "    - [Automold](https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library)\n",
    "    - [Tensorflow Examples](https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/)\n",
    "    - [PIL ImageOps Examples](https://hhsprings.bitbucket.io/docs/programming/examples/python/PIL/ImageOps.html)\n",
    "    \n",
    "- Tensorflow References\n",
    "    - [Using Numpy functions](https://www.tensorflow.org/api_docs/python/tf/numpy_function)\n",
    "    - [Using Python functions](https://www.tensorflow.org/api_docs/python/tf/py_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LfLeMAyQU7z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512 #@param {type:\"integer\"}\n",
    "MOMENTUM = 0.9 #@param {type:\"number\"}\n",
    "LEARNING_RATE = 0.5 #@param {type:\"number\"}\n",
    "WEIGHT_DECAY = 1e-3 #@param {type:\"number\"}\n",
    "EPOCHS = 13 #@param {type:\"integer\"}\n",
    "\n",
    "\n",
    "jobs = int(os.cpu_count()/2)\n",
    "\n",
    "float_rep = \"float16\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.keras.backend.set_epsilon(1e-4)\n",
    "# tf.keras.backend.set_floatx('float16')\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "# tf.random.set_random_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOsYqWHvQBrZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0821 17:47:47.895162 139782799902528 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/eva/assignment-14/lib.py:206: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "W0821 17:47:47.895967 139782799902528 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/eva/assignment-14/lib.py:208: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.0\n",
      "0.15.0\n",
      "1.14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'lib' from '/home/ec2-user/SageMaker/eva/assignment-14/lib.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "print(np.__version__)\n",
    "print(skimage.__version__)\n",
    "\n",
    "\n",
    "import time, math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import gc\n",
    "print(tf.__version__)\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import copy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import os\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "from data_science_utils import misc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "from matplotlib import cm\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,RandomFog,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, ChannelDropout, ChannelShuffle,RandomContrast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0821 17:47:49.074721 139782799902528 <ipython-input-5-bf20c9c65a9e>:1] This is a warning\n"
     ]
    }
   ],
   "source": [
    "logger.info('This is a warning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist/Read Train Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "with open('train_data_s2.pkl', 'wb') as file:\n",
    "    pickle.dump(train_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "with open('train_data_s2.pkl', 'rb') as file:\n",
    "    train_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_append_to_results(filename, result_object=None):\n",
    "    from pathlib import Path\n",
    "    import ast\n",
    "    my_file = Path(filename)\n",
    "    if my_file.is_file():\n",
    "        results = misc.load_list_per_line(filename)\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    if result_object is not None:\n",
    "        results.append(str(result_object))\n",
    "        misc.save_list_per_line(results, filename)\n",
    "    \n",
    "    def lit_eval(r):\n",
    "        try:\n",
    "            return ast.literal_eval(r)\n",
    "        except:\n",
    "            return None\n",
    "    return list(filter(lambda x: x is not None,map(lit_eval,results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,opt,global_step,BATCH_SIZE,EPOCHS,train_data,x_test, y_test,len_train,log_test_acc=False):\n",
    "    len_test = len(x_test)\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE).prefetch(len_test)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    logger.info(\"Starting Training\")\n",
    "    t = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        tf.keras.backend.set_learning_phase(1)    \n",
    "        train_loss = test_loss = train_acc = test_acc = 0.0\n",
    "        train_set = train_data[epoch]\n",
    "        for (x, y) in train_set:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, correct = model(x, y)\n",
    "\n",
    "            var = model.trainable_variables\n",
    "            grads = tape.gradient(loss, var)\n",
    "            for g, v in zip(grads, var):\n",
    "                g += v * (WEIGHT_DECAY/(epoch+1)) * BATCH_SIZE\n",
    "            opt.apply_gradients(zip(grads, var), global_step=global_step)\n",
    "\n",
    "            train_loss += loss.numpy()\n",
    "            train_acc += correct.numpy()\n",
    "            train_accs.append(train_acc / len_train)\n",
    "        if log_test_acc or epoch==EPOCHS-1:    \n",
    "            tf.keras.backend.set_learning_phase(0)\n",
    "            for (x, y) in test_set:\n",
    "                loss, correct = model(x, y)\n",
    "                test_loss += loss.numpy()\n",
    "                test_acc += correct.numpy()\n",
    "            test_accs.append(test_acc / len_test)\n",
    "            logger.debug(msg(\"epoch = %2s\"%epoch,'||train=> loss: %.3f' %(train_loss / len_train), 'acc: %.3f' % (train_acc / len_train), '||val=> loss: %.3f' % (test_loss / len_test), 'val acc: %.3f' %(test_acc / len_test), '%.1fs'%(time.time() - t)))\n",
    "        logger.debug(msg(\"trained for epoch = \",epoch,\"train acc = \",train_accs[-1]))\n",
    "            \n",
    "    time_spent = time.time() - t\n",
    "    logger.info(msg(\"Train acc = \",train_accs[-1],\"Test acc =\",test_accs[-1],\"Time Taken = \",time_spent))\n",
    "    return train_accs[-1],test_accs[-1],time_spent\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenters(da_config,train_mean):\n",
    "    cutout_config = da_config[\"cutout_config\"]\n",
    "    cifar10_augs_config = da_config[\"cifar10_augs_config\"]\n",
    "    hue_config = da_config[\"hue_config\"]\n",
    "    cutout_mapper = get_numpy_wrapper(get_cutout_eraser(**cutout_config),\"cutout\")\n",
    "    cifar10_mapper = get_numpy_wrapper(CIFAR10Policy(**cifar10_augs_config, fillcolor=tuple(train_mean.astype(int)), log=False),\"AutoAug\")\n",
    "    hue_mapper = get_hue_aug(hue_config[\"max_delta\"])\n",
    "    full_wrapper = get_multimapper([hflip_mapper,cutout_mapper,cifar10_mapper,hue_mapper])\n",
    "    logger.debug(\"Augmentation Functions Built\")\n",
    "    return full_wrapper\n",
    "\n",
    "\n",
    "def process_full_augmentation_all_epochs(full_wrapper, EPOCHS, normalize, x_train, y_train):\n",
    "    train_data = {}\n",
    "    len_train = len(x_train)\n",
    "    normalize = get_numpy_wrapper(normalize, Tout=tf.float32)\n",
    "    t = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(full_wrapper, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(len_train).batch(len_train).prefetch(len_train)\n",
    "        i = 0\n",
    "        for x,y in train_set:\n",
    "            xt = x.numpy()\n",
    "            logger.debug(msg(\"X Shape = \",xt.shape[0], \"Train Length =\",len_train))\n",
    "            assert x.numpy().shape[0]==len_train\n",
    "            train_set = (xt,y.numpy())\n",
    "            i = i+1\n",
    "        assert i==1\n",
    "        train_data[epoch] = train_set\n",
    "        logger.debug(msg(\"Augmentation Epoch = \",epoch,\"Time Spent = %.1f\" % (time.time() - t)))\n",
    "        \n",
    "    train_sets = {}\n",
    "    for epoch in range(EPOCHS):\n",
    "        key = (epoch)%(max(train_data.keys())+1)\n",
    "        train_set = train_data[key]\n",
    "        train_set =tf.data.Dataset.from_tensor_slices(train_set).batch(BATCH_SIZE).prefetch(len_train)\n",
    "        train_sets[epoch] = train_set\n",
    "    time_taken = time.time() - t\n",
    "    logger.info(msg(\"Augmentation Done, Time Taken = %.1f\"%(time_taken)))\n",
    "    return train_sets,time_taken\n",
    "    \n",
    "    \n",
    "\n",
    "def fetch_cifar_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "    len_train, len_test = len(x_train), len(x_test)\n",
    "    y_train = y_train.astype('int64').reshape(len_train)\n",
    "    y_test = y_test.astype('int64').reshape(len_test)\n",
    "    train_mean = np.mean(x_train, axis=(0,1,2))\n",
    "    train_std = np.std(x_train, axis=(0,1,2))\n",
    "\n",
    "    def normalize(x):\n",
    "        return ((x.astype('float32') - train_mean) / train_std).astype('float32')\n",
    "\n",
    "    \n",
    "    pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
    "    x_train = pad4(x_train)\n",
    "    x_test = normalize(x_test)\n",
    "    logger.info(\"Data Fetching Done\")\n",
    "    return x_train, y_train, x_test, y_test, classes,len_train,len_test, normalize, train_mean\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "def model_builder(model_config,BATCH_SIZE, len_train):\n",
    "    \n",
    "    batches_per_epoch = len_train//BATCH_SIZE + 1\n",
    "    model = FNet(**model_config[\"model\"])\n",
    "    \n",
    "    enable_olr = model_config[\"optimizer\"][\"enable_olr\"]\n",
    "    enable_momentum = enable_olr\n",
    "    max_lr = model_config[\"optimizer\"][\"max_lr\"]\n",
    "    max_momentum = model_config[\"optimizer\"][\"max_momentum\"]\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    if enable_olr:\n",
    "        mid_epoch = model_config[\"optimizer\"][\"mid_epoch\"]\n",
    "        start_lr = model_config[\"optimizer\"][\"start_lr\"]\n",
    "        \n",
    "        end_lr = model_config[\"optimizer\"][\"end_lr\"]\n",
    "        pre_end_lr = model_config[\"optimizer\"][\"pre_end_lr\"]\n",
    "        pre_end_epoch = model_config[\"optimizer\"][\"pre_end_epoch\"]\n",
    "        \n",
    "        enable_momentum = model_config[\"optimizer\"][\"enable_momentum\"]\n",
    "        if enable_momentum:\n",
    "            min_momentum = model_config[\"optimizer\"][\"min_momentum\"]\n",
    "            momentum_schedule =  lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [max_momentum, min_momentum, max_momentum, max_momentum])[0] \n",
    "            momentum_func = lambda: momentum_schedule(global_step/batches_per_epoch)\n",
    "    \n",
    "        lr_schedule = lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [start_lr, max_lr, pre_end_lr, end_lr])[0] # LR = 0.75\n",
    "        lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE\n",
    "    \n",
    "    \n",
    "    \n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=lr_func if enable_olr else max_lr, momentum=momentum_func if enable_momentum else max_momentum, use_nesterov=True)\n",
    "    logger.info(\"Model Built\")\n",
    "    return model,opt,global_step\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def runner(configs):\n",
    "    # fetch Data\n",
    "    x_train, y_train, x_test, y_test, classes,len_train,len_test, normalize,train_mean = fetch_cifar_data()\n",
    "    prev_da_config = None\n",
    "    train_data = None\n",
    "    results = []\n",
    "    for i,config in enumerate(configs):\n",
    "        reset_keras()\n",
    "        logger.info(\"Testing for Config: %s\",i)\n",
    "        _ = gc.collect()\n",
    "        model_config = config[\"model_config\"]\n",
    "        da_config = config[\"augmentation_config\"]\n",
    "        BATCH_SIZE = config[\"training_config\"][\"BATCH_SIZE\"]\n",
    "        EPOCHS = config[\"training_config\"][\"EPOCHS\"]\n",
    "        save_file = config[\"training_config\"][\"save_file\"]\n",
    "        \n",
    "        same_as_previous_config = prev_da_config == da_config\n",
    "        prev_da_config = da_config\n",
    "        model,opt,global_step = model_builder(model_config,BATCH_SIZE, len_train)\n",
    "        \n",
    "        if not same_as_previous_config:\n",
    "            augmenter = build_augmenters(da_config,train_mean)\n",
    "            train_data,time_taken = process_full_augmentation_all_epochs(augmenter, EPOCHS, normalize, x_train, y_train)\n",
    "            \n",
    "        \n",
    "        train_acc,test_acc,time_spent = run_model(model,opt,global_step,BATCH_SIZE,EPOCHS,train_data,x_test, y_test,len_train,log_test_acc=False)\n",
    "        \n",
    "        result = dict(config=config,results=dict(train=train_acc,test=test_acc,training_time=time_spent,augmentation_time=time_taken))\n",
    "        read_and_append_to_results(save_file, result)\n",
    "        results.append(result)\n",
    "        del model\n",
    "        model = None\n",
    "    logger.info(\"Grid Search Complete, Total Results Count = %s\",len(results))\n",
    "    assert len(results)==len(configs)\n",
    "    return results\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0821 18:43:25.656678 139782799902528 <ipython-input-27-e77edb9b4089>:59] Data Fetching Done\n",
      "I0821 18:43:26.181678 139782799902528 <ipython-input-27-e77edb9b4089>:109] Testing for Config: 0\n",
      "I0821 18:43:26.335893 139782799902528 <ipython-input-27-e77edb9b4089>:95] Model Built\n",
      "I0821 18:52:09.650828 139782799902528 <ipython-input-27-e77edb9b4089>:38]  Augmentation Done, Time Taken = 523.3\n",
      "I0821 18:52:10.229412 139782799902528 <ipython-input-26-198f9c213b46>:10] Starting Training\n",
      "I0821 18:54:56.086319 139782799902528 <ipython-input-26-198f9c213b46>:40]  Train acc =  0.91854 Test acc = 0.9114 Time Taken =  165.85596656799316\n",
      "I0821 18:54:56.241785 139782799902528 <ipython-input-27-e77edb9b4089>:109] Testing for Config: 1\n",
      "I0821 18:54:56.381575 139782799902528 <ipython-input-27-e77edb9b4089>:95] Model Built\n",
      "I0821 18:54:56.467541 139782799902528 <ipython-input-26-198f9c213b46>:10] Starting Training\n",
      "I0821 18:57:45.651390 139782799902528 <ipython-input-26-198f9c213b46>:40]  Train acc =  0.92318 Test acc = 0.9126 Time Taken =  169.18287014961243\n",
      "I0821 18:57:45.815831 139782799902528 <ipython-input-27-e77edb9b4089>:109] Testing for Config: 2\n",
      "I0821 18:57:45.958047 139782799902528 <ipython-input-27-e77edb9b4089>:95] Model Built\n",
      "I0821 18:57:46.182955 139782799902528 <ipython-input-26-198f9c213b46>:10] Starting Training\n",
      "I0821 19:00:35.600674 139782799902528 <ipython-input-26-198f9c213b46>:40]  Train acc =  0.92138 Test acc = 0.9133 Time Taken =  169.41677069664001\n",
      "I0821 19:00:35.763665 139782799902528 <ipython-input-27-e77edb9b4089>:109] Testing for Config: 3\n",
      "I0821 19:00:35.908849 139782799902528 <ipython-input-27-e77edb9b4089>:95] Model Built\n",
      "I0821 19:00:36.074970 139782799902528 <ipython-input-26-198f9c213b46>:10] Starting Training\n",
      "I0821 19:03:25.275370 139782799902528 <ipython-input-26-198f9c213b46>:40]  Train acc =  0.91768 Test acc = 0.9131 Time Taken =  169.19939851760864\n",
      "I0821 19:03:25.431533 139782799902528 <ipython-input-27-e77edb9b4089>:109] Testing for Config: 4\n",
      "I0821 19:03:25.570822 139782799902528 <ipython-input-27-e77edb9b4089>:95] Model Built\n",
      "I0821 19:03:25.793609 139782799902528 <ipython-input-26-198f9c213b46>:10] Starting Training\n",
      "I0821 19:06:15.508686 139782799902528 <ipython-input-26-198f9c213b46>:40]  Train acc =  0.92272 Test acc = 0.9144 Time Taken =  169.71405935287476\n",
      "I0821 19:06:15.668170 139782799902528 <ipython-input-27-e77edb9b4089>:109] Testing for Config: 5\n",
      "I0821 19:06:15.813541 139782799902528 <ipython-input-27-e77edb9b4089>:95] Model Built\n",
      "I0821 19:06:16.038342 139782799902528 <ipython-input-26-198f9c213b46>:10] Starting Training\n",
      "I0821 19:09:05.175993 139782799902528 <ipython-input-26-198f9c213b46>:40]  Train acc =  0.91602 Test acc = 0.9051 Time Taken =  169.1365134716034\n",
      "I0821 19:09:05.193537 139782799902528 <ipython-input-27-e77edb9b4089>:133] Grid Search Complete, Total Results Count = 6\n"
     ]
    }
   ],
   "source": [
    "augmentation_config = dict(cutout_config=dict(s_l=0.04, s_h=0.06,max_erasures_per_image=1),\n",
    "                           hue_config=dict(max_delta=0.1),\n",
    "                           cifar10_augs_config=dict(proba=0.5, enabled_policies=[(\"rotate\",5, 15,),(\"shearX\",0.05, 0.15,),(\"shearY\",0.05, 0.15,)]))\n",
    "\n",
    "model_config = dict(model=dict(start_kernels=64,sparse_bn=True,thin_block=False),\n",
    "                    optimizer=dict(enable_olr=True,max_lr=0.5,\n",
    "                                   start_lr=0.001,pre_end_lr=0.01,end_lr=0.005,\n",
    "                                   mid_epoch=5,pre_end_epoch=13,\n",
    "                                   max_momentum=0.9,enable_momentum=False))\n",
    "\n",
    "training_config = dict(BATCH_SIZE=512,EPOCHS=15,save_file=\"results.txt\")\n",
    "\n",
    "test_config = dict(augmentation_config=augmentation_config,model_config=model_config,training_config=training_config)\n",
    "\n",
    "test_config_2 = copy.deepcopy(test_config)\n",
    "test_config_2[\"model_config\"][\"optimizer\"][\"enable_momentum\"] = True\n",
    "test_config_2[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.95\n",
    "test_config_2[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.85\n",
    "\n",
    "test_config_3 = copy.deepcopy(test_config)\n",
    "test_config_3[\"model_config\"][\"optimizer\"][\"enable_momentum\"] = True\n",
    "test_config_3[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.95\n",
    "test_config_3[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.85\n",
    "test_config_3[\"model_config\"][\"optimizer\"][\"max_lr\"] = 0.4\n",
    "\n",
    "\n",
    "test_config_4 = copy.deepcopy(test_config)\n",
    "test_config_4[\"model_config\"][\"optimizer\"][\"enable_momentum\"] = True\n",
    "test_config_4[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.95\n",
    "test_config_4[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.85\n",
    "test_config_4[\"model_config\"][\"optimizer\"][\"pre_end_epoch\"] = 12\n",
    "\n",
    "test_config_5 = copy.deepcopy(test_config)\n",
    "test_config_5[\"model_config\"][\"optimizer\"][\"enable_momentum\"] = True\n",
    "test_config_5[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.92\n",
    "test_config_5[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.82\n",
    "\n",
    "test_config_6 = copy.deepcopy(test_config)\n",
    "test_config_6[\"model_config\"][\"optimizer\"][\"enable_momentum\"] = True\n",
    "test_config_6[\"model_config\"][\"optimizer\"][\"max_momentum\"] = 0.98\n",
    "test_config_6[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.88\n",
    "\n",
    "results = runner([test_config,test_config_2,test_config_3,test_config_4,test_config_5,test_config_6])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'config': {'augmentation_config': {'cutout_config': {'s_l': 0.04,\n",
       "     's_h': 0.06,\n",
       "     'max_erasures_per_image': 1},\n",
       "    'hue_config': {'max_delta': 0.1},\n",
       "    'cifar10_augs_config': {'proba': 0.5,\n",
       "     'enabled_policies': [('rotate', 5, 15),\n",
       "      ('shearX', 0.1, 0.2),\n",
       "      ('shearY', 0.1, 0.2)]}},\n",
       "   'model_config': {'model': {'start_kernels': 64,\n",
       "     'sparse_bn': True,\n",
       "     'thin_block': False},\n",
       "    'optimizer': {'enable_olr': True,\n",
       "     'max_lr': 0.5,\n",
       "     'start_lr': 0.001,\n",
       "     'pre_end_lr': 0.01,\n",
       "     'end_lr': 0.005,\n",
       "     'mid_epoch': 5,\n",
       "     'pre_end_epoch': 13,\n",
       "     'max_momentum': 0.95,\n",
       "     'enable_momentum': True,\n",
       "     'min_momentum': 0.85}},\n",
       "   'training_config': {'BATCH_SIZE': 512,\n",
       "    'EPOCHS': 15,\n",
       "    'save_file': 'results.txt'}},\n",
       "  'results': {'train': 0.85312,\n",
       "   'test': 0.8749,\n",
       "   'training_time': 174.3953332901001,\n",
       "   'augmentation_time': 522.2028503417969}},\n",
       " {'config': {'augmentation_config': {'cutout_config': {'s_l': 0.04,\n",
       "     's_h': 0.06,\n",
       "     'max_erasures_per_image': 1},\n",
       "    'hue_config': {'max_delta': 0.1},\n",
       "    'cifar10_augs_config': {'proba': 0.5,\n",
       "     'enabled_policies': [('rotate', 5, 15),\n",
       "      ('shearX', 0.1, 0.2),\n",
       "      ('shearY', 0.1, 0.2)]}},\n",
       "   'model_config': {'model': {'start_kernels': 64,\n",
       "     'sparse_bn': True,\n",
       "     'thin_block': False},\n",
       "    'optimizer': {'enable_olr': True,\n",
       "     'max_lr': 0.5,\n",
       "     'start_lr': 0.001,\n",
       "     'pre_end_lr': 0.01,\n",
       "     'end_lr': 0.005,\n",
       "     'mid_epoch': 5,\n",
       "     'pre_end_epoch': 13,\n",
       "     'max_momentum': 0.9,\n",
       "     'enable_momentum': False}},\n",
       "   'training_config': {'BATCH_SIZE': 512,\n",
       "    'EPOCHS': 15,\n",
       "    'save_file': 'results.txt'}},\n",
       "  'results': {'train': 0.91536,\n",
       "   'test': 0.9107,\n",
       "   'training_time': 166.04180312156677,\n",
       "   'augmentation_time': 522.2028503417969}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'config': {'augmentation_config': {'cutout_config': {'s_l': 0.04,\n",
       "     's_h': 0.06,\n",
       "     'max_erasures_per_image': 1},\n",
       "    'hue_config': {'max_delta': 0.1},\n",
       "    'cifar10_augs_config': {'proba': 0.5,\n",
       "     'enabled_policies': [('rotate', 5, 15),\n",
       "      ('shearX', 0.1, 0.2),\n",
       "      ('shearY', 0.1, 0.2)]}},\n",
       "   'model_config': {'model': {'start_kernels': 64,\n",
       "     'sparse_bn': True,\n",
       "     'thin_block': True},\n",
       "    'optimizer': {'enable_olr': True,\n",
       "     'max_lr': 0.5,\n",
       "     'start_lr': 0.001,\n",
       "     'pre_end_lr': 0.01,\n",
       "     'end_lr': 0.005,\n",
       "     'mid_epoch': 5,\n",
       "     'pre_end_epoch': 13,\n",
       "     'max_momentum': 0.9,\n",
       "     'enable_momentum': False}},\n",
       "   'training_config': {'BATCH_SIZE': 512,\n",
       "    'EPOCHS': 15,\n",
       "    'save_file': 'results.txt'}},\n",
       "  'results': {'train': 0.87154,\n",
       "   'test': 0.8858,\n",
       "   'training_time': 150.91348099708557,\n",
       "   'augmentation_time': 432.3216440677643}}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "DN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
