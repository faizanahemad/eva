{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/faizanahemad/eva/blob/master/assignment-14/DN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tried**\n",
    "- Alternate Augmentations\n",
    "    - AutoAugment\n",
    "    - Cutout\n",
    "    - hue/saturation/brightness\n",
    "- Used DataSets.map for building augmentors\n",
    "- Used Slanted OLR with Slowdown\n",
    "- GridSearch on Model+Training+Augmentation with Result storage in text file\n",
    "- Momentum Scheduling like OLR.\n",
    "\n",
    "**Results**:\n",
    "\n",
    "```\n",
    "{'train': 0.9651333333333333,\n",
    "  'test': 0.9350333333333333,\n",
    "  'training_time': 379.8242540359497,\n",
    "  'augmentation_time': 769.7456877231598}\n",
    "  \n",
    "{'train': 0.9452866666666666,\n",
    "  'test': 0.9206,\n",
    "  'training_time': 172.4951605796814,\n",
    "  'augmentation_time': 478.07853603363037},\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources**\n",
    "\n",
    "- XLA\n",
    "\n",
    "    - [Google Notebook example](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/tutorials/xla_compile.ipynb)\n",
    "\n",
    "    - [Blog](https://medium.com/@xianbao.qian/use-xla-with-keras-3ca5d0309c26)\n",
    "    - [Example Enabling XLA](https://github.com/tensorflow/models/blob/7212436440eaa11293ca84befcc5d8327109ea76/official/utils/misc/keras_utils.py#L158)\n",
    "    \n",
    "- Mixed Precision\n",
    "\n",
    "- Augmentation Libs\n",
    "    - [imgaug](https://github.com/aleju/imgaug)\n",
    "    - [albumentations](https://github.com/albu/albumentations)\n",
    "    - [Automold](https://github.com/UjjwalSaxena/Automold--Road-Augmentation-Library)\n",
    "    - [Tensorflow Examples](https://www.wouterbulten.nl/blog/tech/data-augmentation-using-tensorflow-data-dataset/)\n",
    "    - [PIL ImageOps Examples](https://hhsprings.bitbucket.io/docs/programming/examples/python/PIL/ImageOps.html)\n",
    "    \n",
    "- Tensorflow References\n",
    "    - [Using Numpy functions](https://www.tensorflow.org/api_docs/python/tf/numpy_function)\n",
    "    - [Using Python functions](https://www.tensorflow.org/api_docs/python/tf/py_function)\n",
    "    - [Tensorflow Data](https://www.tensorflow.org/datasets/catalog/overview)\n",
    "    - [Dataset to TFRecord](https://github.com/tensorflow/tensorflow/issues/16926)\n",
    "    - [TFRecord](https://www.tensorflow.org/tutorials/load_data/tf_records#tfrecords_format_details)\n",
    "    - [TFRecord Load](https://www.tensorflow.org/tutorials/load_data/images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LfLeMAyQU7z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "WEIGHT_DECAY = 1e-3 #@param {type:\"number\"}\n",
    "\n",
    "\n",
    "\n",
    "jobs = int(os.cpu_count()/2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.keras.backend.set_epsilon(1e-4)\n",
    "# tf.keras.backend.set_floatx('float16')\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "# tf.random.set_random_seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOsYqWHvQBrZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17.0\n",
      "0.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0826 18:04:43.328066 140607039358784 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/eva/assignment-14/lib.py:24: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n",
      "W0826 18:04:43.329121 140607039358784 deprecation_wrapper.py:119] From /home/ec2-user/SageMaker/eva/assignment-14/lib.py:26: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "print(np.__version__)\n",
    "print(skimage.__version__)\n",
    "\n",
    "\n",
    "import time, math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import gc\n",
    "print(tf.__version__)\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import copy\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import os\n",
    "from importlib import reload\n",
    "import lib\n",
    "reload(lib)\n",
    "from lib import *\n",
    "from data_science_utils import misc\n",
    "from data_science_utils.vision.keras.regularizers import get_cutout_eraser\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "from matplotlib import cm\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,RandomFog,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, ChannelDropout, ChannelShuffle,RandomContrast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 18:04:43.451470 140607039358784 <ipython-input-5-bf20c9c65a9e>:1] This is a warning\n"
     ]
    }
   ],
   "source": [
    "logger.info('This is a warning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_append_to_results(filename, result_object=None, read=False):\n",
    "    from pathlib import Path\n",
    "    import ast\n",
    "    my_file = Path(filename)\n",
    "    if my_file.is_file():\n",
    "        results = misc.load_list_per_line(filename)\n",
    "    else:\n",
    "        results = []\n",
    "    \n",
    "    results = list(filter(lambda x:len(x)>2,results))\n",
    "    \n",
    "    if result_object is not None:\n",
    "        results.append(result_object)\n",
    "        misc.save_list_per_line(results, filename)\n",
    "        \n",
    "    if read:\n",
    "        def lit_eval(r):\n",
    "            try:\n",
    "                return ast.literal_eval(r)\n",
    "            except:\n",
    "                return None\n",
    "        return list(filter(lambda x: x is not None,map(lit_eval,results)))\n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model,opt,global_step,BATCH_SIZE,EPOCHS,train_data,x_test, y_test,len_train,log_test_acc=False):\n",
    "    len_test = len(x_test)\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    test_set = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE).prefetch(len_test)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    logger.debug(\"Starting Training\")\n",
    "    t = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        tf.keras.backend.set_learning_phase(1)    \n",
    "        train_loss = test_loss = train_acc = test_acc = 0.0\n",
    "        train_set = train_data[epoch]\n",
    "        for (x, y) in train_set:\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, correct = model(x, y)\n",
    "\n",
    "            var = model.trainable_variables\n",
    "            grads = tape.gradient(loss, var)\n",
    "            for g, v in zip(grads, var):\n",
    "                g += v * (WEIGHT_DECAY/(epoch+1)) * BATCH_SIZE\n",
    "            opt.apply_gradients(zip(grads, var), global_step=global_step)\n",
    "#             opt.apply_gradients(zip(grads, var))\n",
    "\n",
    "            train_loss += loss.numpy()\n",
    "            train_acc += correct.numpy()\n",
    "            train_accs.append(train_acc / len_train)\n",
    "        if log_test_acc or epoch==EPOCHS-1:    \n",
    "            tf.keras.backend.set_learning_phase(0)\n",
    "            for (x, y) in test_set:\n",
    "                loss, correct = model(x, y)\n",
    "                test_loss += loss.numpy()\n",
    "                test_acc += correct.numpy()\n",
    "            test_accs.append(test_acc / len_test)\n",
    "            logger.debug(msg(\"epoch = %2s\"%epoch,'||train=> loss: %.3f' %(train_loss / len_train), 'acc: %.3f' % (train_acc / len_train), '||val=> loss: %.3f' % (test_loss / len_test), 'val acc: %.3f' %(test_acc / len_test), '%.1fs'%(time.time() - t)))\n",
    "        logger.debug(msg(\"trained for epoch = \",epoch,\"train acc = \",train_accs[-1]))\n",
    "            \n",
    "    time_spent = time.time() - t\n",
    "    logger.debug(msg(\"Train acc = \",train_accs[-1],\"Test acc =\",test_accs[-1],\"Time Taken = \",time_spent))\n",
    "    return train_accs[-1],test_accs[-1],time_spent\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# da_config = {\"max_augmentations_per_img\":2,\"augmentations\":[{\"augmentation_type\":{\"params,proba\"}}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmenters(da_config,train_mean):\n",
    "    \n",
    "    cutout_config = da_config[\"cutout_config\"]\n",
    "    cutout_mapper = get_numpy_wrapper(get_cutout_eraser(**cutout_config),\"cutout\")\n",
    "    augmentations = [hflip_mapper,cutout_mapper]\n",
    "    if \"cifar10_augs_config\" in da_config:\n",
    "        cifar10_augs_config = da_config[\"cifar10_augs_config\"]\n",
    "        if type(cifar10_augs_config)==dict:\n",
    "            cifar10_mapper = get_numpy_wrapper(CIFAR10Policy(**cifar10_augs_config, fillcolor=tuple(train_mean.astype(int)), log=False),\"AutoAug\")\n",
    "            augmentations.append(cifar10_mapper)\n",
    "        else:\n",
    "            assert type(cifar10_augs_config)==list\n",
    "            for cfc in cifar10_augs_config:\n",
    "                cifar10_mapper = get_numpy_wrapper(CIFAR10Policy(**cfc, fillcolor=tuple(train_mean.astype(int)), log=False),\"AutoAug\")\n",
    "                augmentations.append(cifar10_mapper)\n",
    "    if \"hue_config\" in da_config:\n",
    "        hue_config = da_config[\"hue_config\"]\n",
    "        hue_mapper = get_hue_aug(hue_config[\"max_delta\"])\n",
    "        augmentations.append(hue_mapper)\n",
    "    \n",
    "    full_wrapper = get_multimapper(augmentations)\n",
    "    logger.debug(\"Augmentation Functions Built\")\n",
    "    return full_wrapper\n",
    "\n",
    "\n",
    "def process_full_augmentation_all_epochs(full_wrapper, EPOCHS,BATCH_SIZE, normalize, x_train, y_train):\n",
    "    train_data = {}\n",
    "    len_train = len(x_train)\n",
    "    normalize = get_numpy_wrapper(normalize, Tout=tf.float32)\n",
    "    t = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_set = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(full_wrapper, num_parallel_calls=jobs).map(normalize, num_parallel_calls=jobs).shuffle(len_train).batch(len_train).prefetch(len_train)\n",
    "        i = 0\n",
    "        for x,y in train_set:\n",
    "            xt = x.numpy()\n",
    "            logger.debug(msg(\"X Shape = \",xt.shape[0], \"Train Length =\",len_train))\n",
    "            assert x.numpy().shape[0]==len_train\n",
    "            train_set = (xt,y.numpy())\n",
    "            i = i+1\n",
    "        assert i==1\n",
    "        train_data[epoch] = train_set\n",
    "        logger.debug(msg(\"Augmentation Epoch = \",epoch,\"Time Spent = %.1f\" % (time.time() - t)))\n",
    "        \n",
    "    train_sets = {}\n",
    "    for epoch in range(EPOCHS):\n",
    "        key = (epoch)%(max(train_data.keys())+1)\n",
    "        train_set = train_data[key]\n",
    "        train_set =tf.data.Dataset.from_tensor_slices(train_set).batch(BATCH_SIZE).prefetch(len_train).cache()\n",
    "        train_sets[epoch] = train_set\n",
    "    time_taken = time.time() - t\n",
    "    logger.info(msg(\"Augmentation Done for EPOCHS = %s, Time Taken = %.1f\"%(EPOCHS,time_taken)))\n",
    "    return train_sets,time_taken\n",
    "    \n",
    "    \n",
    "\n",
    "def fetch_cifar_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    classes = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "    len_train, len_test = len(x_train), len(x_test)\n",
    "    y_train = y_train.astype('int64').reshape(len_train)\n",
    "    y_test = y_test.astype('int64').reshape(len_test)\n",
    "    train_mean = np.mean(x_train, axis=(0,1,2))\n",
    "    train_std = np.std(x_train, axis=(0,1,2))\n",
    "\n",
    "    def normalize(x):\n",
    "        return ((x.astype('float32') - train_mean) / train_std).astype('float32')\n",
    "\n",
    "    \n",
    "    pad4 = lambda x: np.pad(x, [(0, 0), (4, 4), (4, 4), (0, 0)], mode='reflect')\n",
    "    x_train = pad4(x_train)\n",
    "    x_test = normalize(x_test)\n",
    "    logger.info(\"Data Fetching Done\")\n",
    "    return x_train, y_train, x_test, y_test, classes,len_train,len_test, normalize, train_mean\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "def model_builder(model_config,EPOCHS,BATCH_SIZE, len_train):\n",
    "    \n",
    "    batches_per_epoch = len_train//BATCH_SIZE + 1\n",
    "    model = FNet(**model_config[\"model\"])\n",
    "    \n",
    "    enable_olr = model_config[\"optimizer\"][\"enable_olr\"]\n",
    "    enable_momentum = enable_olr\n",
    "    max_lr = model_config[\"optimizer\"][\"max_lr\"]\n",
    "    max_momentum = model_config[\"optimizer\"][\"max_momentum\"]\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    if enable_olr:\n",
    "        mid_epoch = model_config[\"optimizer\"][\"mid_epoch\"]\n",
    "        start_lr = model_config[\"optimizer\"][\"start_lr\"]\n",
    "        \n",
    "        end_lr = model_config[\"optimizer\"][\"end_lr\"]\n",
    "        pre_end_lr = model_config[\"optimizer\"][\"pre_end_lr\"]\n",
    "        pre_end_epoch = model_config[\"optimizer\"][\"pre_end_epoch\"]\n",
    "        \n",
    "        enable_momentum = model_config[\"optimizer\"][\"enable_momentum\"]\n",
    "        if enable_momentum:\n",
    "            min_momentum = model_config[\"optimizer\"][\"min_momentum\"]\n",
    "            momentum_schedule =  lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [max_momentum, min_momentum, max_momentum, max_momentum])[0] \n",
    "            momentum_func = lambda: momentum_schedule(global_step/batches_per_epoch)\n",
    "            \n",
    "            \n",
    "    \n",
    "        lr_schedule = lambda t: np.interp([t], [0, mid_epoch, pre_end_epoch, EPOCHS], [start_lr, max_lr, pre_end_lr, end_lr])[0] # LR = 0.75\n",
    "        lr_func = lambda: lr_schedule(global_step/batches_per_epoch)/BATCH_SIZE\n",
    "\n",
    "    opt = tf.train.MomentumOptimizer(learning_rate=lr_func if enable_olr else max_lr, momentum=momentum_func if enable_momentum else max_momentum, use_nesterov=True)\n",
    "    logger.debug(\"Model Built\")\n",
    "    return model,opt,global_step\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def runner(configs,cv=3):\n",
    "    # fetch Data\n",
    "    x_train, y_train, x_test, y_test, classes,len_train,len_test, normalize,train_mean = fetch_cifar_data()\n",
    "    prev_da_config = None\n",
    "    train_data = None\n",
    "    results = []\n",
    "    logger.info(\"Started GridSearch, Total Configs to Test = %s\"%len(configs))\n",
    "    reset_keras()\n",
    "    t = time.time()\n",
    "    prev_epochs = 0\n",
    "    for i,config in enumerate(configs):\n",
    "        \n",
    "        _ = gc.collect()\n",
    "        model_config = config[\"model_config\"]\n",
    "        da_config = config[\"augmentation_config\"]\n",
    "        BATCH_SIZE = config[\"training_config\"][\"BATCH_SIZE\"]\n",
    "        EPOCHS = config[\"training_config\"][\"EPOCHS\"]\n",
    "        save_file = config[\"training_config\"][\"save_file\"]\n",
    "        \n",
    "        \n",
    "        same_as_previous_config = prev_da_config == da_config and EPOCHS==prev_epochs\n",
    "        prev_da_config = da_config\n",
    "        prev_epochs = EPOCHS\n",
    "        \n",
    "        if not same_as_previous_config:\n",
    "            augmenter = build_augmenters(da_config,train_mean)\n",
    "            train_data,time_taken = process_full_augmentation_all_epochs(augmenter, EPOCHS,BATCH_SIZE, normalize, x_train, y_train)\n",
    "            \n",
    "        train_acc = test_acc = time_spent = 0.0\n",
    "        for j in range(cv):\n",
    "            model,opt,global_step = model_builder(model_config,EPOCHS,BATCH_SIZE, len_train)\n",
    "            t1_acc,t2_acc,tsp = run_model(model,opt,global_step,BATCH_SIZE,EPOCHS,train_data,x_test, y_test,len_train,log_test_acc=False)\n",
    "            train_acc += t1_acc\n",
    "            test_acc += t2_acc\n",
    "            time_spent += tsp\n",
    "            reset_keras()\n",
    "        train_acc /= cv\n",
    "        test_acc /= cv\n",
    "        time_spent /= cv\n",
    "        \n",
    "        logger.info(msg(\"Trial =\",i,\"Train acc = \",train_acc,\"Test acc =\",test_acc,\"Time Taken = \",time_spent, \"Model Params = %.2f\"%(model.count_params()/1e6)))\n",
    "            \n",
    "        \n",
    "        result = dict(config=config,results=dict(train=train_acc,test=test_acc,training_time=time_spent,augmentation_time=time_taken))\n",
    "        read_and_append_to_results(save_file, result)\n",
    "        results.append(result)\n",
    "        del model\n",
    "        model = None\n",
    "    logger.info(\"Grid Search Complete, Total Results Count = %s, Time Taken = %.1f\",len(results),(time.time()-t))\n",
    "    assert len(results)==len(configs)\n",
    "    return results\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with Less Epochs\n",
    "augmentation_config = dict(cutout_config=dict(s_l=0.03, s_h=0.04,max_erasures_per_image=1,proba=0.5))\n",
    "\n",
    "model_config = dict(model=dict(start_kernels=32,sparse_bn=True,thin_block=False,\n",
    "                               enable_skip=True,enable_pool_before_skip=True, \n",
    "                               no_activation_first_conv=False, residual_dropout=0.0, spatial_dropout=0.05),\n",
    "                    optimizer=dict(enable_olr=True,max_lr=0.75,\n",
    "                                   start_lr=0.02,pre_end_lr=0.08,end_lr=0.02,\n",
    "                                   mid_epoch=2,pre_end_epoch=6,\n",
    "                                   max_momentum=0.9,min_momentum=0.8, enable_momentum=True,))\n",
    "\n",
    "training_config = dict(BATCH_SIZE=512,EPOCHS=7,save_file=\"results-trails.txt\")\n",
    "\n",
    "trail_conf = dict(augmentation_config=augmentation_config,model_config=model_config,training_config=training_config)\n",
    "\n",
    "\n",
    "results = runner([trail_conf],cv=3)\n",
    "\n",
    "results\n",
    "\n",
    "# 0.8744666666666667,0.8758333333333334 = gmp,\n",
    "# 0.8753666666666667, 0.8725999999999999, 0.8727333333333332"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0826 18:12:47.283362 140607039358784 <ipython-input-9-893880cce488>:72] Data Fetching Done\n",
      "I0826 18:12:47.284206 140607039358784 <ipython-input-9-893880cce488>:121] Started GridSearch, Total Configs to Test = 2\n",
      "I0826 18:25:32.231678 140607039358784 <ipython-input-9-893880cce488>:51]  Augmentation Done for EPOCHS = 24, Time Taken = 764.7\n",
      "W0826 18:25:36.728163 140607039358784 deprecation.py:323] From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:3940: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "I0826 18:44:34.168587 140607039358784 <ipython-input-9-893880cce488>:155]  Trial = 0 Train acc =  0.9626733333333334 Test acc = 0.9330333333333334 Time Taken =  380.04483993848163 Model Params = 6.60\n",
      "I0826 19:03:31.984951 140607039358784 <ipython-input-9-893880cce488>:155]  Trial = 1 Train acc =  0.9617866666666667 Test acc = 0.9331 Time Taken =  378.91440478960675 Model Params = 6.60\n",
      "I0826 19:03:31.989938 140607039358784 <ipython-input-9-893880cce488>:163] Grid Search Complete, Total Results Count = 2, Time Taken = 3044.6\n"
     ]
    }
   ],
   "source": [
    "augmentation_config = dict(cutout_config=dict(s_l=0.04, s_h=0.06,max_erasures_per_image=1),\n",
    "                           hue_config=dict(max_delta=0.05),\n",
    "                           cifar10_augs_config=dict(proba=0.5, enabled_policies=[(\"shearX\",0.05, 0.1,),(\"shearY\",0.05, 0.1,)]))\n",
    "\n",
    "model_config = dict(model=dict(start_kernels=64,sparse_bn=True,thin_block=False,\n",
    "                               enable_skip=True,enable_pool_before_skip=True, \n",
    "                               no_activation_first_conv=False, residual_dropout=0.0, spatial_dropout=0.0),\n",
    "                    optimizer=dict(enable_olr=True,max_lr=0.6,\n",
    "                                   start_lr=0.01,pre_end_lr=0.04,end_lr=0.02,\n",
    "                                   mid_epoch=5,pre_end_epoch=14,\n",
    "                                   max_momentum=0.9,min_momentum=0.8, enable_momentum=True,))\n",
    "\n",
    "training_config = dict(BATCH_SIZE=512,EPOCHS=15,save_file=\"results.txt\")\n",
    "\n",
    "test_config_0 = dict(augmentation_config=augmentation_config,model_config=model_config,training_config=training_config)\n",
    "test_config_0[\"augmentation_config\"][\"cifar10_augs_config\"][\"enabled_policies\"] = [(\"shearX\",0.05, 0.1,),(\"shearY\",0.05, 0.1,)]\n",
    "test_config_0[\"model_config\"][\"model\"][\"spatial_dropout\"] = 0.02\n",
    "test_config_0[\"model_config\"][\"model\"][\"sparse_bn\"] = False\n",
    "test_config_0[\"training_config\"][\"EPOCHS\"] = 24\n",
    "test_config_0[\"model_config\"][\"optimizer\"][\"mid_epoch\"] = 6\n",
    "test_config_0[\"model_config\"][\"optimizer\"][\"pre_end_epoch\"] = 22\n",
    "test_config_0[\"model_config\"][\"optimizer\"][\"max_lr\"] = 0.7\n",
    "test_config_0[\"model_config\"][\"optimizer\"][\"min_momentum\"] = 0.7\n",
    "\n",
    "test_config_1 = copy.deepcopy(test_config_0)\n",
    "test_config_1[\"model_config\"][\"optimizer\"][\"mid_epoch\"] = 4\n",
    "\n",
    "\n",
    "\n",
    "results = runner([test_config_0,test_config_1],cv=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Results from Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[{'train': 0.9301733333333333,\n",
       "  'test': 0.9179333333333334,\n",
       "  'training_time': 199.6779501438141,\n",
       "  'augmentation_time': 447.832466840744},\n",
       " {'train': 0.9244533333333335,\n",
       "  'test': 0.9194,\n",
       "  'training_time': 210.19939549763998,\n",
       "  'augmentation_time': 496.33520460128784},\n",
       " {'train': 0.9313733333333333,\n",
       "  'test': 0.9208,\n",
       "  'training_time': 195.69241094589233,\n",
       "  'augmentation_time': 478.4510440826416},\n",
       " {'train': 0.9452866666666666,\n",
       "  'test': 0.9206,\n",
       "  'training_time': 172.4951605796814,\n",
       "  'augmentation_time': 478.07853603363037},\n",
       " {'train': 0.9315066666666668,\n",
       "  'test': 0.9198333333333334,\n",
       "  'training_time': 198.11389915148416,\n",
       "  'augmentation_time': 478.07853603363037},\n",
       " {'train': 0.9396800000000001,\n",
       "  'test': 0.9222,\n",
       "  'training_time': 197.06325793266296,\n",
       "  'augmentation_time': 478.07853603363037},\n",
       " {'train': 0.9441333333333333,\n",
       "  'test': 0.9175000000000001,\n",
       "  'training_time': 172.49557248751321,\n",
       "  'augmentation_time': 478.07853603363037},\n",
       " {'train': 0.94476,\n",
       "  'test': 0.9200333333333334,\n",
       "  'training_time': 172.38328536351523,\n",
       "  'augmentation_time': 478.07853603363037},\n",
       " {'train': 0.9394866666666667,\n",
       "  'test': 0.9207666666666666,\n",
       "  'training_time': 198.5017306804657,\n",
       "  'augmentation_time': 478.89523124694824},\n",
       " {'train': 0.9362266666666667,\n",
       "  'test': 0.9194666666666667,\n",
       "  'training_time': 198.81704982121786,\n",
       "  'augmentation_time': 478.89523124694824},\n",
       " {'train': 0.9421266666666668,\n",
       "  'test': 0.9198666666666666,\n",
       "  'training_time': 196.2002325852712,\n",
       "  'augmentation_time': 478.89523124694824},\n",
       " {'train': 0.91872,\n",
       "  'test': 0.9159,\n",
       "  'training_time': 199.66006708145142,\n",
       "  'augmentation_time': 592.9354138374329},\n",
       " {'train': 0.9234866666666667,\n",
       "  'test': 0.9161,\n",
       "  'training_time': 173.50740257898966,\n",
       "  'augmentation_time': 592.9354138374329},\n",
       " {'train': 0.96368,\n",
       "  'test': 0.9290333333333333,\n",
       "  'training_time': 314.6755781173706,\n",
       "  'augmentation_time': 769.4746196269989},\n",
       " {'train': 0.9346933333333333,\n",
       "  'test': 0.9171666666666667,\n",
       "  'training_time': 199.57181660334268,\n",
       "  'augmentation_time': 527.5470242500305},\n",
       " {'train': 0.93486,\n",
       "  'test': 0.9148666666666667,\n",
       "  'training_time': 174.8775749206543,\n",
       "  'augmentation_time': 527.5470242500305},\n",
       " {'train': 0.9651333333333333,\n",
       "  'test': 0.9350333333333333,\n",
       "  'training_time': 379.8242540359497,\n",
       "  'augmentation_time': 769.7456877231598},\n",
       " {'train': 0.9628266666666666,\n",
       "  'test': 0.9338333333333333,\n",
       "  'training_time': 378.9603673617045,\n",
       "  'augmentation_time': 769.7456877231598},\n",
       " {'train': 0.9660133333333333,\n",
       "  'test': 0.9347666666666666,\n",
       "  'training_time': 378.89605617523193,\n",
       "  'augmentation_time': 769.7456877231598},\n",
       " {'train': 0.9661933333333333,\n",
       "  'test': 0.9344,\n",
       "  'training_time': 380.73613079388934,\n",
       "  'augmentation_time': 769.7456877231598}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_and_append_to_results(\"results.txt\", read=True))\n",
    "[x[\"results\"] for x in read_and_append_to_results(\"results.txt\", read=True)[-20:]]\n",
    "\n",
    "# Augmentations per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "DN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
